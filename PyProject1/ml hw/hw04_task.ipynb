{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Случайные леса\n",
    "__Суммарное количество баллов: 10__\n",
    "\n",
    "__Решение отправлять на `ml.course.practice@gmail.com`__\n",
    "\n",
    "__Тема письма: `[ML][HW04] <ФИ>`, где вместо `<ФИ>` указаны фамилия и имя__\n",
    "\n",
    "В этом задании вам предстоит реализовать ансамбль деревьев решений, известный как случайный лес, применить его к публичным данным пользователей социальной сети Вконтакте, и сравнить его эффективность с ансамблем, предоставляемым библиотекой CatBoost.\n",
    "\n",
    "В результате мы сможем определить, какие подписки пользователей больше всего влияют на определение возраста и пола человека. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'catboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-03c757368fa5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmath\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msqrt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mcatboost\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCatBoostClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'catboost'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import copy\n",
    "from math import sqrt\n",
    "from catboost import CatBoostClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(x):\n",
    "    _, counts = np.unique(x, return_counts=True)\n",
    "    proba = counts / len(x)\n",
    "    return np.sum(proba * (1 - proba))\n",
    "    \n",
    "def entropy(x):\n",
    "    _, counts = np.unique(x, return_counts=True)\n",
    "    proba = counts / len(x)\n",
    "    return -np.sum(proba * np.log2(proba))\n",
    "\n",
    "def gain(left_y, right_y, criterion):\n",
    "    y = np.concatenate((left_y, right_y))\n",
    "    return criterion(y) - (criterion(left_y) * len(left_y) + criterion(right_y) * len(right_y)) / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1 (2 балла)\n",
    "Random Forest состоит из деревьев решений. Каждое такое дерево строится на одной из выборок, полученных при помощи bagging. Элементы, которые не вошли в новую обучающую выборку, образуют out-of-bag выборку. Кроме того, в каждом узле дерева мы случайным образом выбираем набор из `max_features` и ищем признак для предиката разбиения только в этом наборе.\n",
    "\n",
    "Сегодня мы будем работать только с бинарными признаками, поэтому нет необходимости выбирать значение признака для разбиения.\n",
    "\n",
    "#### Методы\n",
    "`predict(X)` - возвращает предсказанные метки для элементов выборки `X`\n",
    "\n",
    "#### Параметры конструктора\n",
    "`X, y` - обучающая выборка и соответствующие ей метки классов. Из нее нужно получить выборку для построения дерева при помощи bagging. Out-of-bag выборку нужно запомнить, она понадобится потом.\n",
    "\n",
    "`criterion=\"gini\"` - задает критерий, который будет использоваться при построении дерева. Возможные значения: `\"gini\"`, `\"entropy\"`.\n",
    "\n",
    "`max_depth=None` - ограничение глубины дерева. Если `None` - глубина не ограничена\n",
    "\n",
    "`min_samples_leaf=1` - минимальное количество элементов в каждом листе дерева.\n",
    "\n",
    "`max_features=\"auto\"` - количество признаков, которые могут использоваться в узле. Если `\"auto\"` - равно `sqrt(X.shape[1])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeNode:\n",
    "    def __init__(self, dim, left, right):\n",
    "        self.dim = dim\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "\n",
    "class DecisionTreeLeaf:\n",
    "    def __init__(self, y):\n",
    "        self.y = y\n",
    "        labels, count = np.unique(y, return_counts=True)\n",
    "        self.label = labels[np.argmax(count)]\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, X, y, criterion=\"gini\", max_depth=None, min_samples_leaf=1, max_features=None):\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_features = max_features\n",
    "        if max_features is None:\n",
    "            self.max_features = int(sqrt(len(X[0])))\n",
    "        if criterion == \"gini\":\n",
    "            self.criterion = gini\n",
    "        if criterion == \"entropy\":\n",
    "            self.criterion = entropy\n",
    "        self.root = self.makeNode(np.array(list(zip(y, X))), 0)\n",
    "\n",
    "    def gain(self, left_y, right_y, criterion):\n",
    "        if len(left_y) < self.min_samples_leaf or len(right_y) < self.min_samples_leaf:\n",
    "            return -1\n",
    "        else:\n",
    "            return gain(left_y, right_y, criterion)\n",
    "\n",
    "    def makeNode(self, X_y, depth):\n",
    "        if depth == self.max_depth or len(X_y) < 2 * self.min_samples_leaf:\n",
    "            return DecisionTreeLeaf(X_y[:, 0])\n",
    "        all_features = np.array(list(range(len(X_y[0][1]))))\n",
    "        np.random.shuffle(all_features)\n",
    "        features = all_features[:self.max_features]\n",
    "        gains = [self.gain(np.array([z[0] for z in filter(lambda x: x[1][d] == 0, X_y)]),\n",
    "                           np.array([z[0] for z in filter(lambda x: x[1][d] == 1, X_y)]),\n",
    "                           self.criterion) for d in features]\n",
    "        dim = features[np.argmax(gains)]\n",
    "        max_gain = max(gains)\n",
    "        if max_gain <= 0:\n",
    "            return DecisionTreeLeaf(X_y[:, 0])\n",
    "        else:\n",
    "            left = np.array(list(filter(lambda x: x[1][dim] == 0, X_y)))\n",
    "            right = np.array(list(filter(lambda x: x[1][dim] == 1, X_y)))\n",
    "            return DecisionTreeNode(dim, self.makeNode(left, depth + 1),\n",
    "                                    self.makeNode(right, depth + 1))\n",
    "\n",
    "    def find_x(self, tree, x):\n",
    "        if isinstance(tree, DecisionTreeLeaf):\n",
    "            return tree.label\n",
    "        else:\n",
    "            if x[tree.dim] == 0:\n",
    "                return self.find_x(tree.left, x)\n",
    "            else:\n",
    "                return self.find_x(tree.right, x)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return [self.find_x(self.root, x) for x in X]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2 (2 балла)\n",
    "Теперь реализуем сам Random Forest. Идея очень простая: строим `n` деревьев, а затем берем модальное предсказание.\n",
    "\n",
    "#### Параметры конструктора\n",
    "`n_estimators` - количество используемых для предсказания деревьев.\n",
    "\n",
    "Остальное - параметры деревьев.\n",
    "\n",
    "#### Методы\n",
    "`fit(X, y)` - строит `n_estimators` деревьев по выборке `X`.\n",
    "\n",
    "`predict(X)` - для каждого элемента выборки `X` возвращает самый частый класс, который предсказывают для него деревья."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestClassifier:\n",
    "    def __init__(self, criterion=\"gini\", max_depth=None, min_samples_leaf=1, max_features=\"auto\", n_estimators=10,\n",
    "                 ratio=1.0):\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        if max_features == \"auto\":\n",
    "            self.max_features = int(sqrt(len(X[0])))\n",
    "        self.n_estimators = n_estimators\n",
    "        self.ratio = ratio\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.trees = None\n",
    "        self.oob = None\n",
    "        self.m = None\n",
    "\n",
    "    def importance(self):\n",
    "        importance = np.array([0.0 for j in range(len(X[0]))])\n",
    "        for n in range(self.n_estimators):\n",
    "            X_oob = np.array([self.X[i] for i in self.oob[n]])\n",
    "            y_oob = np.array([self.y[i] for i in self.oob[n]])\n",
    "            err_obb = len(list(filter(lambda x: x[0] == x[1], zip(self.trees[n].predict(X_oob), y_oob)))) / len(y_oob)\n",
    "            for j in range(len(X[0])):\n",
    "                col_j = copy.copy(X_oob[:, j])\n",
    "                np.random.shuffle(col_j)\n",
    "                X_oob_j = copy.copy(X_oob)\n",
    "                X_oob_j[:, j] = col_j\n",
    "                err_j = len(list(filter(lambda x: x[0] == x[1], zip(self.trees[n].predict(X_oob_j), y_oob)))) / len(\n",
    "                    y_oob)\n",
    "                importance[j] += err_obb - err_j\n",
    "        return importance / self.n_estimators\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.m = int(self.ratio * len(X))\n",
    "\n",
    "        def baggingTree():\n",
    "            randind = np.random.randint(len(X), size=self.m)\n",
    "            tree = DecisionTree([X[i] for i in randind], [y[i] for i in randind], criterion=self.criterion,\n",
    "                                min_samples_leaf=self.min_samples_leaf,\n",
    "                                max_depth=self.max_depth, max_features=self.max_features)\n",
    "            out_of_bag = np.array(list(filter(lambda x: x not in randind, range(len(X)))))\n",
    "            return tree, out_of_bag\n",
    "\n",
    "        trees_oob = np.array([baggingTree() for n in range(self.n_estimators)])\n",
    "        self.trees = trees_oob[:, 0]\n",
    "        self.oob = trees_oob[:, 1]\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds = np.transpose(np.array([tree.predict(X) for tree in self.trees]))\n",
    "\n",
    "        def best(predictions):\n",
    "            labels, count = np.unique(predictions, return_counts=True)\n",
    "            return labels[np.argmax(count)]\n",
    "\n",
    "        return [best(y) for y in preds]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3 (2 балла)\n",
    "Часто хочется понимать, насколько большую роль играет тот или иной признак для предсказания класса объекта. Есть различные способы посчитать его важность. Один из простых способов сделать это для Random Forest - посчитать out-of-bag ошибку предсказания `err_oob`, а затем перемешать значения признака `j` и посчитать ее (`err_oob_j`) еще раз. Оценкой важности признака `j` для одного дерева будет разность `err_oob_j - err_oob`, важность для всего леса считается как среднее значение важности по деревьям.\n",
    "\n",
    "Реализуйте функцию `feature_importance`, которая принимает на вход Random Forest и возвращает массив, в котором содержится важность для каждого признака."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance(rfc):\n",
    "    return rfc.importance()\n",
    "\n",
    "\n",
    "def most_important_features(importance, names, k=20):\n",
    "    idicies = np.argsort(importance)[::-1][:k]\n",
    "    return np.array(names)[idicies]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, пришло время протестировать наше дерево на простом синтетическом наборе данных. В результате должна получиться точность `1.0`, наибольшее значение важности должно быть у признака с индексом `4`, признаки с индексами `2` и `3`  должны быть одинаково важны, а остальные признаки - не важны совсем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Importance: [ 0.00083672 -0.00086362  0.15637837  0.15830291  0.3348981   0.00282328]\n"
     ]
    }
   ],
   "source": [
    "def synthetic_dataset(size):\n",
    "    X = [(np.random.randint(0, 2), np.random.randint(0, 2), int(i % 6 == 3), \n",
    "          int(i % 6 == 0), int(i % 3 == 2), np.random.randint(0, 2)) for i in range(size)]\n",
    "    y = [i % 3 for i in range(size)]\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = synthetic_dataset(1000)\n",
    "rfc = RandomForestClassifier(n_estimators=100)\n",
    "rfc.fit(X, y)\n",
    "print(\"Accuracy:\", np.mean(rfc.predict(X) == y))\n",
    "print(\"Importance:\", feature_importance(rfc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4 (1 балл)\n",
    "Теперь поработаем с реальными данными.\n",
    "\n",
    "Выборка состоит из публичных анонимизированных данных пользователей социальной сети Вконтакте. Первые два столбца отражают возрастную группу (`zoomer`, `doomer` и `boomer`) и пол (`female`, `male`). Все остальные столбцы являются бинарными признаками, каждый из них определяет, подписан ли пользователь на определенную группу/публичную страницу или нет.\\\n",
    "\\\n",
    "Необходимо обучить два классификатора, один из которых определяет возрастную группу, а второй - пол.\\\n",
    "\\\n",
    "Эксперименты с множеством используемых признаков и подбор гиперпараметров приветствуются. Лес должен строиться за какое-то разумное время."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(path):\n",
    "    dataframe = pandas.read_csv(path, header=0)\n",
    "    dataset = dataframe.values.tolist()\n",
    "    random.shuffle(dataset)\n",
    "    y_age = [row[0] for row in dataset]\n",
    "    y_sex = [row[1] for row in dataset]\n",
    "    X = [row[2:] for row in dataset]\n",
    "    \n",
    "    return np.array(X), np.array(y_age), np.array(y_sex), list(dataframe.columns)[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y_age, y_sex, features = read_dataset(\"vk.csv\")\n",
    "X_train, X_test, y_age_train, y_age_test, y_sex_train, y_sex_test = train_test_split(X, y_age, y_sex, train_size=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Возраст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7414880201765448\n",
      "Most important features:\n",
      "1. ovsyanochan\n",
      "2. 4ch\n",
      "3. rhymes\n",
      "4. styd.pozor\n",
      "5. mudakoff\n",
      "6. dayvinchik\n",
      "7. rapnewrap\n",
      "8. reflexia_our_feelings\n",
      "9. bot_maxim\n",
      "10. pravdashowtop\n",
      "11. iwantyou\n",
      "12. pixel_stickers\n",
      "13. pozor\n",
      "14. tumblr_vacuum\n",
      "15. leprum\n",
      "16. i_des\n",
      "17. ohhluul\n",
      "18. exclusive_muzic\n",
      "19. rem_shkola\n",
      "20. ne.poverish\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=10)\n",
    "\n",
    "rfc.fit(X_train, y_age_train)\n",
    "print(\"Accuracy:\", np.mean(rfc.predict(X_test) == y_age_test))\n",
    "print(\"Most important features:\")\n",
    "for i, name in enumerate(most_important_features(feature_importance(rfc), features, 20)):\n",
    "    print(str(i+1) + \".\", name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пол"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8511979823455234\n",
      "Most important features:\n",
      "1. 40kg\n",
      "2. modnailru\n",
      "3. 9o_6o_9o\n",
      "4. girlmeme\n",
      "5. recipes40kg\n",
      "6. mudakoff\n",
      "7. 4ch\n",
      "8. be.women\n",
      "9. femalemem\n",
      "10. zerofat\n",
      "11. bot_maxim\n",
      "12. cook_good\n",
      "13. i_d_t\n",
      "14. igm\n",
      "15. be.beauty\n",
      "16. reflexia_our_feelings\n",
      "17. rapnewrap\n",
      "18. woman.blog\n",
      "19. thesmolny\n",
      "20. sh.cook\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=10)\n",
    "rfc.fit(X_train, y_sex_train)\n",
    "print(\"Accuracy:\", np.mean(rfc.predict(X_test) == y_sex_test))\n",
    "print(\"Most important features:\")\n",
    "for i, name in enumerate(most_important_features(feature_importance(rfc), features, 20)):\n",
    "    print(str(i+1) + \".\", name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost\n",
    "В качестве аьтернативы попробуем CatBoost. \n",
    "\n",
    "Устаниовить его можно просто с помощью `pip install catboost`. Туториалы можно найти, например, [здесь](https://catboost.ai/docs/concepts/python-usages-examples.html#multiclassification) и [здесь](https://github.com/catboost/tutorials/blob/master/python_tutorial.ipynb). Главное - не забудьте использовать `loss_function='MultiClass'`.\\\n",
    "\\\n",
    "Сначала протестируйте CatBoost на синтетических данных. Выведите точность и важность признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CatBoostClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-f5e44d71d7bd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msynthetic_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m cbc = CatBoostClassifier(iterations=100,\n\u001b[0m\u001b[0;32m      3\u001b[0m                          \u001b[0mloss_function\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'MultiClass'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                          \u001b[0mtask_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"GPU\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                          )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CatBoostClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "X, y = synthetic_dataset(1000)\n",
    "cbc = CatBoostClassifier(iterations=100,\n",
    "                         loss_function='MultiClass',\n",
    "                         task_type=\"GPU\"\n",
    "                         )\n",
    "cbc.fit(X, y)\n",
    "print(\"Accuracy:\", np.mean(cbc.predict(X).reshape((len(y))) == y))\n",
    "print(\"Importance:\", cbc.get_feature_importance())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 5 (3 балла)\n",
    "Попробуем применить один из используемых на практике алгоритмов. В этом нам поможет CatBoost. Также, как и реализованный ними RandomForest, применим его для определения пола и возраста пользователей сети Вконтакте, выведите названия наиболее важных признаков так же, как в задании 3.\\\n",
    "\\\n",
    "Эксперименты с множеством используемых признаков и подбор гиперпараметров приветствуются."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y_age, y_sex, features = read_dataset(\"vk.csv\")\n",
    "X_train, X_test, y_age_train, y_age_test, y_sex_train, y_sex_test = train_test_split(X, y_age, y_sex, train_size=0.9)\n",
    "X_train, X_eval, y_age_train, y_age_eval, y_sex_train, y_sex_eval = train_test_split(X_train, y_age_train, y_sex_train, train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Возраст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CatBoostClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-3c55e8070fef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m cbc = CatBoostClassifier(iterations=100,\n\u001b[0m\u001b[0;32m      2\u001b[0m                          \u001b[0mloss_function\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'MultiClass'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                          \u001b[0mtask_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"GPU\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                          )\n\u001b[0;32m      5\u001b[0m \u001b[0mcbc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_age_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CatBoostClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "cbc = CatBoostClassifier(iterations=100,\n",
    "                         loss_function='MultiClass',\n",
    "                         task_type=\"GPU\"\n",
    "                         )\n",
    "cbc.fit(X_train, y_age_train)\n",
    "print(\"Accuracy:\", np.mean(cbc.predict(X_test).reshape((len(y_age_test))) == y_age_test))\n",
    "print(\"Most important features:\")\n",
    "for i, name in enumerate(most_important_features(cbc.get_feature_importance(), features, 10)):\n",
    "    print(str(i+1) + \".\", name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пол"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CatBoostClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-2fd06890050f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m cbc = CatBoostClassifier(iterations=100,\n\u001b[0m\u001b[0;32m      2\u001b[0m                          \u001b[0mloss_function\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'MultiClass'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                          \u001b[0mtask_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"GPU\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                          )\n\u001b[0;32m      5\u001b[0m \u001b[0mcbc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_sex_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CatBoostClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "cbc = CatBoostClassifier(iterations=100,\n",
    "                         loss_function='MultiClass',\n",
    "                         task_type=\"GPU\"\n",
    "                         )\n",
    "cbc.fit(X_train, y_sex_train)\n",
    "print(\"Accuracy:\", np.mean(cbc.predict(X_test).reshape((len(y_sex_test))) == y_sex_test))\n",
    "print(\"Most important features:\")\n",
    "for i, name in enumerate(most_important_features(cbc.get_feature_importance(), features, 10)):\n",
    "    print(str(i+1) + \".\", name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
