{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 2\n",
    "### Андреев Никита"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.optimize as opt\n",
    "import sklearn.datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Определим функцию $\\sigma$:\n",
    "\n",
    "## $\\sigma_w(x)=\\frac{e^{w^tx}}{1+e^{w^tx}}=\\frac{1}{1+e^{-w^tx}}$\n",
    "\n",
    "Тогда\n",
    "$1-\\sigma_w(x)=1-\\frac{1}{1+e^{-w^tx}}=\\frac{1}{1+e^{w^tx}}=\\sigma_w(-x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Функция, которую необходимо минимизировать:\n",
    "## $f(w)=-\\frac{1}{N}\\sum\\limits_1^n\\log(\\sigma_w(y_i x_i))$, $y_i =\\pm 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\nabla\\sigma_w(x)=\\frac{e^{-w^tx}}{(1+e^{-w^tx})^2}x = \\sigma_w(x) \\sigma_w(-x)x$\n",
    "\n",
    "### $\\nabla f(w) = -\\frac{1}{N}\\sum\\limits_1^n\\sigma_w(-y_i x_i)y_i x_i$\n",
    "\n",
    "### $\\nabla^2 f(w) = \\frac{1}{N}\\sum\\limits_1^n\\sigma_w(y_i x_i)\\sigma_w(-y_i x_i)x_i x_i^t=\n",
    "\\frac{1}{N}\\sum\\limits_1^n\\sigma_w(x_i)\\sigma_w(x_i)x_i x_i^t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "def sigma(w, x):\n",
    "    return 1 / (1 + np.exp(-np.dot(w, x)))\n",
    "\n",
    "\n",
    "def function(w, X):\n",
    "    return -1 / len(X) * np.sum([np.log(sigma(w, (2 * l - 1) * x)) for x, l in zip(X, labels)])\n",
    "\n",
    "\n",
    "def gradient(w, X):\n",
    "    return -1 / len(X) * np.sum([x * (l - sigma(w, x)) for x, l in zip(X, labels)], axis=0)\n",
    "\n",
    "\n",
    "def gessian(w, dw1, dw2, X):\n",
    "    return -1 / len(X) * np.sum([np.dot(dw1, x) * np.dot(dw2, x) * ((sigma(w, x) - 1) * sigma(w, x)) for x in X])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [1 4]]\n"
     ]
    }
   ],
   "source": [
    "def der(fun, point, epsilon):\n",
    "    return (fun(point + epsilon) - fun(point)) / epsilon\n",
    "\n",
    "\n",
    "def check_gradient(fun, grad, X, R, diff_eps=np.sqrt(sys.float_info.epsilon)):\n",
    "    dim = X.shape[1]\n",
    "    w = np.random.random(dim)\n",
    "    w = (2 * w - 1) * R\n",
    "    dw = np.eye(dim)\n",
    "    difs = [\n",
    "        np.abs((np.dot(grad(w, X), dw_i) - der(lambda t: fun(w + t * dw_i, X), 0, diff_eps)) / np.dot(grad(w, X), dw_i))\n",
    "        for dw_i in dw]\n",
    "    return np.average(difs)\n",
    "\n",
    "\n",
    "def check_gessian(grad, gess, X, R, diff_eps=np.sqrt(sys.float_info.epsilon)):\n",
    "    dim = X.shape[1]\n",
    "    w = np.random.random(dim)\n",
    "    w = (2 * w - 1) * R\n",
    "    dw = np.eye(dim)\n",
    "    difs = [\n",
    "        np.abs(\n",
    "            (gess(w, dw1, dw2, X) - der(lambda t: np.dot(grad(w + t * dw1, X), dw2), 0, diff_eps))\n",
    "            / gess(w, dw1, dw2, X))\n",
    "        for dw1 in dw for dw2 in dw]\n",
    "    return np.average(difs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def golden_search_bounded(fun, a0, b0, eps=0.0001):\n",
    "    ratio = (1 + 5 ** 0.5) / 2\n",
    "\n",
    "    def step(a, b, c, fc):\n",
    "        if b - a < eps:\n",
    "            return a, fun(a)\n",
    "        else:\n",
    "            d = a + b - c\n",
    "            fd = fun(d)\n",
    "            if c > d:\n",
    "                c, d = d, c\n",
    "                fc, fd = fd, fc\n",
    "            if fc < fd:\n",
    "                return step(a, d, c, fc)\n",
    "            else:\n",
    "                return step(c, b, d, fd)\n",
    "\n",
    "    c0 = a0 + (b0 - a0) / ratio\n",
    "    return step(a0, b0, c0, fun(c0))[0]\n",
    "\n",
    "\n",
    "def golden_search(fun, b=300, a=0, eps=0.0001):\n",
    "    x = golden_search_bounded(fun, a, b, eps)\n",
    "    if np.abs(x - a) < eps:\n",
    "        return golden_search(fun, a, 2 * a - b)\n",
    "    if np.abs(x - b) < eps:\n",
    "        return golden_search(fun, 2 * b - a, b)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(fun, grad, one_dim_search, start, epsilon):\n",
    "    x = start\n",
    "    d0 = grad(x)\n",
    "    d = grad(x)\n",
    "    k = 0\n",
    "    while np.dot(d, d) / np.dot(d0, d0) > epsilon:\n",
    "        x1 = x - d * one_dim_search(lambda alpha: fun(x - d * alpha))\n",
    "        if k % 10 == 0:\n",
    "            print(np.linalg.norm(x - x1))\n",
    "        x = x1\n",
    "        d = grad(x)\n",
    "        k += 1\n",
    "    return x, k"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
